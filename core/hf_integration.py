import logging
from typing import Dict, Any, Optional
from transformers import pipeline, AutoTokenizer, GenerationConfig
from huggingface_hub import login, snapshot_download
from pathlib import Path
import torch

class HFIntegration:
    """
    ÙˆØ­Ø¯Ø© Ù…ØªÙƒØ§Ù…Ù„Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Hugging Face
    """
    def __init__(self, 
                model_name: str, 
                hf_token: str, 
                cache_dir: str = "models",
                device: Optional[str] = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ÙˆØ­Ø¯Ø©
        
        :param model_name: Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ HF (Ù…Ø«Ø§Ù„: google/gemma-7b-it)
        :param hf_token: ØªÙˆÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ù…Ù† HF
        :param cache_dir: Ù…Ø³Ø§Ø± ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: models/)
        :param device: Ø¬Ù‡Ø§Ø² Ø§Ù„ØªØ´ØºÙŠÙ„ (cuda/cpu/auto)ØŒ None Ù„Ù„ÙƒØ´Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        """
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        self.hf_token = hf_token
        self.cache_dir = Path(cache_dir)
        self.device = self._determine_device(device)
        self.model = None
        self.tokenizer = None
        self._setup()

    def _determine_device(self, device: Optional[str]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø¬Ù‡Ø§Ø² Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø«Ù„"""
        if device:
            return device
        return "cuda" if torch.cuda.is_available() else "cpu"

    def _setup(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø¨ÙŠØ¦Ø©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨ØµÙ„Ø§Ø­ÙŠØ§Øª ÙƒØ§Ù…Ù„Ø©
            self.cache_dir.mkdir(exist_ok=True, parents=True)
            
            # ØªØ³Ø¬ÙŠÙ„ Ø¯Ø®ÙˆÙ„ Ø¢Ù…Ù†
            login(token=self.hf_token, add_to_git_credential=True)
            self.logger.info("âœ… ØªÙ… Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ù…Ø¹ Hugging Face Hub")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self._load_model()
            
        except Exception as e:
            self.logger.critical(f"ÙØ´Ù„ Ø­Ø±Ø¬ ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯: {e}", exc_info=True)
            raise RuntimeError("ØªØ¹Ø°Ø± ØªÙ‡ÙŠØ¦Ø© ÙˆØ­Ø¯Ø© HF") from e

    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¢Ù…Ù† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            self.logger.info(f"âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {self.model_name}...")
            
            # ØªØ­Ù…ÙŠÙ„ Tokenizer Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token,
                cache_dir=str(self.cache_dir),
                trust_remote_code=True
            )
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
            generation_config = GenerationConfig.from_pretrained(
                self.model_name,
                token=self.hf_token
            )
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.model = pipeline(
                task="text-generation",
                model=self.model_name,
                token=self.hf_token,
                device=self.device,
                torch_dtype=torch.float16 if "cuda" in self.device else None,
                model_kwargs={
                    "cache_dir": str(self.cache_dir),
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True
                },
                generation_config=generation_config
            )
            
            self.logger.info("ğŸ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            self.logger.error(f"ğŸ”¥ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}", exc_info=True)
            raise

    def generate_text(self, 
                    prompt: str, 
                    generation_params: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡
        
        :param prompt: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„
        :param generation_params: Ù…Ø¹Ø§Ù…Ù„Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
        :return: Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
            - text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯
            - status: success/error
            - metrics: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        """
        default_params = {
            "max_new_tokens": 300,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True,
            "num_return_sequences": 1
        }
        
        params = {**default_params, **(generation_params or {})}
        
        try:
            self.logger.debug(f"ğŸ“ Ù…Ø¹Ø§Ù„Ø¬Ø© Prompt (Ø£ÙˆÙ„ 100 Ø­Ø±Ù): {prompt[:100]}...")
            
            start_time = time.time()
            outputs = self.model(prompt, **params)
            latency = time.time() - start_time
            
            result = {
                "text": outputs[0]["generated_text"],
                "status": "success",
                "model": self.model_name,
                "metrics": {
                    "latency_seconds": round(latency, 2),
                    "tokens_generated": len(outputs[0]["generated_text"].split()),
                    "device": self.device
                }
            }
            
            self.logger.info(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ {result['metrics']['tokens_generated']} token ÙÙŠ {result['metrics']['latency_seconds']} Ø«Ø§Ù†ÙŠØ©")
            return result
            
        except Exception as e:
            error_msg = f"âŒ ÙØ´Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return {
                "status": "error",
                "message": error_msg,
                "model": self.model_name
            }

    def __del__(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡"""
        try:
            if hasattr(self, 'model'):
                del self.model
            if hasattr(self, 'tokenizer'):
                del self.tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
        except Exception as e:
            self.logger.warning(f"ØªØ­Ø°ÙŠØ± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {e}")

# Ø¥Ø¶Ø§ÙØ© Ù…ÙÙŠØ¯Ø© Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¢Ù…Ù†
def create_hf_integration(model_name: str, hf_token: str, **kwargs) -> Optional[HFIntegration]:
    """Ø¯Ø§Ù„Ø© Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¢Ù…Ù†"""
    try:
        return HFIntegration(model_name, hf_token, **kwargs)
    except Exception as e:
        logging.error(f"ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ HFIntegration: {e}")
        return None